{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.pretokenization_example import find_chunk_boundaries\n",
    "import regex as re\n",
    "\n",
    "sample_tiny_path = \"/Users/prateekmahadevappahavanur/Documents/GitHub/test_task/assignment1-basics/tests/fixtures/tinystories_sample.txt\"\n",
    "\n",
    "\n",
    "def bpe_train(file_path,vocab_size,special_tokens):\n",
    "    \n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "    special_token_pattern = \"|\".join(re.escape(token) for token in special_tokens)\n",
    "    word_freqs = {}\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(\n",
    "            f, 2, \"<|endoftext|>\".encode(\"utf-8\"))\n",
    "            \n",
    "        # The following is a serial implementation, but you can parallelize this \n",
    "        # by sending each start/end pair to a set of processes.\n",
    "        word_freqs = {}\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "            f.seek(start)\n",
    "            chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "            text = re.split(special_token_pattern, chunk)\n",
    "            for segment in text:\n",
    "                if not segment:\n",
    "                    continue\n",
    "                for match in re.finditer(PAT, segment):\n",
    "                    token = match.group()\n",
    "                    token_bytes = tuple(token.encode(\"utf-8\"))\n",
    "                    # print(token,\"---\",token_bytes)\n",
    "                    word_freqs[token_bytes] = word_freqs.get(token_bytes, 0) + 1\n",
    "\n",
    "    def get_count(mapped_list):\n",
    "        count = {}\n",
    "        for test_list,value in mapped_list.items():\n",
    "            test_list = list(test_list)\n",
    "            for a1,a2 in zip(test_list,test_list[1:]):\n",
    "                count[(a1,a2)] = count.get((a1,a2),0) +  value\n",
    "        return count\n",
    "\n",
    "\n",
    "    def merge_pair(test_list,pair,replacement):\n",
    "        i = 0\n",
    "        new_list = []\n",
    "        while i < len(test_list):\n",
    "            if i < len(test_list) - 1 and test_list[i] == pair[0] and test_list[i+1] == pair[1]:\n",
    "                new_list.append(replacement)\n",
    "                i = i+2\n",
    "            else:\n",
    "                new_list.append(test_list[i])\n",
    "                i += 1\n",
    "        return new_list\n",
    "\n",
    "    def get_token_bytes(token_id, vocab_list):\n",
    "        \"\"\"Get the bytes for a token ID\"\"\"\n",
    "        if token_id < 256:\n",
    "            return bytes([token_id])  # Single byte\n",
    "        else:\n",
    "            return vocab_list[token_id]  # Already merged token\n",
    "        \n",
    "    def make_comparable_pair(pair, vocab_list):\n",
    "        \"\"\"Convert pair elements to comparable format\"\"\"\n",
    "        a, b = pair\n",
    "        # Convert to bytes for consistent comparison\n",
    "        if isinstance(a, int):\n",
    "            a_bytes = bytes([a]) if a < 256 else f\"merged_{a}\".encode()\n",
    "        else:\n",
    "            a_bytes = a\n",
    "        if isinstance(b, int):\n",
    "            b_bytes = bytes([b]) if b < 256 else f\"merged_{b}\".encode()\n",
    "        else:\n",
    "            b_bytes = b\n",
    "        return (a_bytes, b_bytes)\n",
    "\n",
    "    # Initialize vocabulary\n",
    "    vocab_list = []\n",
    "    # Special tokens first\n",
    "    for special_token in special_tokens:\n",
    "        vocab_list.append(special_token.encode(\"utf-8\"))\n",
    "    # All 256 bytes\n",
    "    for i in range(256):\n",
    "        vocab_list.append(bytes([i]))\n",
    "\n",
    "    num_merges = vocab_size - len(special_tokens) - 256\n",
    "\n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        # print(f\"----iteration {i}-----\")\n",
    "        pair_counts = get_count(word_freqs)\n",
    "        # Find best pair with lexicographic tie-breaking\n",
    "        best_pair = max(\n",
    "                pair_counts.items(),\n",
    "                key=lambda x: (x[1], make_comparable_pair(x[0], vocab_list)),\n",
    "            )[0]\n",
    "        best_pair\n",
    "\n",
    "        new_token_id = len(vocab_list)\n",
    "\n",
    "        first_bytes = get_token_bytes(best_pair[0], vocab_list)\n",
    "        second_bytes = get_token_bytes(best_pair[1], vocab_list)\n",
    "        merged_bytes = first_bytes + second_bytes\n",
    "\n",
    "        vocab_list.append(merged_bytes)\n",
    "\n",
    "        # print(merged_bytes)\n",
    "\n",
    "        new_word_freqs = {}\n",
    "        for words,freq in word_freqs.items():\n",
    "            merged_list = merge_pair(words, best_pair, new_token_id)\n",
    "            new_word_freqs[tuple(merged_list)] = freq\n",
    "\n",
    "        word_freqs = new_word_freqs\n",
    "        merges.append(best_pair)\n",
    "\n",
    "\n",
    "    # Build final outputs\n",
    "    vocab_dict = {i: token for i, token in enumerate(vocab_list)}\n",
    "\n",
    "    # Build merges_bytes with proper lookup\n",
    "    merges_bytes = []\n",
    "    for first, second in merges:\n",
    "        first_bytes = get_token_bytes(first, vocab_list)\n",
    "        second_bytes = get_token_bytes(second, vocab_list)\n",
    "        merges_bytes.append((first_bytes, second_bytes))\n",
    "\n",
    "    return vocab_dict,merges_bytes\n",
    "\n",
    "vocab_dict,merges_bytes = bpe_train(sample_tiny_path,300,[\"<|endoftext|>\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '{'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 98\u001b[0m\n\u001b[1;32m     93\u001b[0m         text \u001b[38;5;241m=\u001b[39m full_bytes\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m---> 98\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../tests/fixtures/gpt2_vocab.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../tests/fixtures/gpt2_merges.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|endoftext|>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|endoftext|><|endoftext|>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36mTokenizer.from_file\u001b[0;34m(cls, vocab_path, merges_path, special_tokens)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, vocab_path, merges_path, special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(vocab_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 13\u001b[0m         vocab \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(merges_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     15\u001b[0m         merges \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '{'."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import regex as re\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, merges, special_tokens=None):\n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab_dict = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, vocab_path, merges_path, special_tokens=None):\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            vocab = pickle.load(f)\n",
    "        with open(merges_path, \"rb\") as f:\n",
    "            merges = pickle.load(f)\n",
    "        \n",
    "        # Create and return a new instance\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "    def encode(self, text):\n",
    "\n",
    "        if self.special_tokens is None:\n",
    "            special_token_pattern = []\n",
    "\n",
    "        else:\n",
    "            special_token_pattern = \"|\".join(re.escape(token) for token in self.special_tokens)\n",
    "            text = re.split(f\"({special_token_pattern})\", text)\n",
    "\n",
    "        PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "\n",
    "        words = []\n",
    "        for segment in text:\n",
    "            if not segment:\n",
    "                continue\n",
    "            if self.special_tokens and segment in self.special_tokens:\n",
    "                # Handle special tokens - they should be encoded as single tokens\n",
    "                special_token_bytes = segment.encode(\"utf-8\")\n",
    "                words.append([special_token_bytes])\n",
    "            else:\n",
    "                for match in re.finditer(PAT, segment):\n",
    "                    token = match.group()\n",
    "                    words.append([bytes([x]) for x in token.encode(\"utf-8\")])\n",
    "\n",
    "        final_words = []\n",
    "        for word in words:            \n",
    "            while True:\n",
    "                best_merge = None\n",
    "                best_pos = -1\n",
    "\n",
    "                for i in range(len(word) - 1):\n",
    "                    pair = (word[i], word[i + 1])\n",
    "                    if pair in self.merges:\n",
    "                        merge_priority = self.merges.index(pair)\n",
    "                        if best_merge is None or merge_priority < best_merge[1]:\n",
    "                            best_merge = (i, merge_priority)\n",
    "                            best_pos = i\n",
    "\n",
    "                if best_merge is None:\n",
    "                    break\n",
    "\n",
    "                # Apply the best merge\n",
    "                word = (\n",
    "                    word[:best_pos]\n",
    "                    + [word[best_pos] + word[best_pos + 1]]\n",
    "                    + word[best_pos + 2 :]\n",
    "                )\n",
    "            final_words.append(word)\n",
    "\n",
    "        # print(final_words)\n",
    "\n",
    "        tokenized_words = []     # print(word)\n",
    "        for word in final_words:\n",
    "            tokenized_word = []\n",
    "            for i in word:\n",
    "                tokenized_word.append(self.vocab_dict.get(i,\"<UNK>\"))\n",
    "            tokenized_words.extend(tokenized_word)\n",
    "        return tokenized_words\n",
    "    \n",
    "    def encode_iterable(self, iterable):\n",
    "        for item in iterable:\n",
    "            yield self.encode(item)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"Decode token IDs back to text\"\"\"\n",
    "        # Step 1: Convert IDs to tokens\n",
    "        tokens = [self.vocab.get(token_id,\"<UNK>\")for token_id in tokens]\n",
    "        \n",
    "        # Step 2: Concatenate all bytes\n",
    "        full_bytes = b''.join(tokens)\n",
    "        \n",
    "        # Step 3: Decode to string\n",
    "        text = full_bytes.decode('utf-8')\n",
    "        \n",
    "        return text\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"gpt2_vocab.json\", \"../data/results/merges.pkl\",[\"<|endoftext|>\", \"<|endoftext|><|endoftext|>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HÃ©llÃ² hÃ´w <|endoftext|><|endoftext|> are Ã¼? ðŸ™ƒ<|endoftext|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"HÃ©llÃ² hÃ´w <|endoftext|><|endoftext|> are Ã¼? ðŸ™ƒ<|endoftext|>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pickle\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "vocab_path = \"../data/results/vocab.pkl\"\n",
    "merges_path = \"../data/results/merges.pkl\"\n",
    "\n",
    "with open(merges_path, \"rb\") as f:\n",
    "    merges = pickle.load(f)\n",
    "\n",
    "with open(vocab_path, \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# print(vocab)\n",
    "\n",
    "vocab_dict = {v: k for k, v in vocab.items()}\n",
    "\n",
    "chunk = \"Hello, world! This is a test. <|endoftext|>\"\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "\n",
    "special_token_pattern = \"|\".join(re.escape(token) for token in special_tokens)\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "text = re.split(f\"({special_token_pattern})\", chunk)\n",
    "\n",
    "words = []\n",
    "for segment in text:\n",
    "    if not segment:\n",
    "        continue\n",
    "    if segment in special_tokens:\n",
    "        continue\n",
    "    else:\n",
    "        for match in re.finditer(PAT, segment):\n",
    "            token = match.group()\n",
    "            words.append([bytes([x]) for x in token.encode(\"utf-8\")])\n",
    "\n",
    "final_words = []\n",
    "for word in words:\n",
    "    # print(\"processing word\",word,\"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        i = 0\n",
    "        merge_found = False\n",
    "        while i < len(word) - 1:\n",
    "            if (word[i],word[i+1]) in merges:\n",
    "                word = word[:i] + [word[i] + word[i+1]] + word[i+2:]\n",
    "                merge_found = True\n",
    "                break\n",
    "            i = i+1\n",
    "        if not merge_found:\n",
    "            break\n",
    "    final_words.append(word)\n",
    "\n",
    "# print(final_words)\n",
    "\n",
    "tokenized_words = []     # print(word)\n",
    "for word in final_words:\n",
    "    tokenized_word = []\n",
    "    for i in word:\n",
    "        tokenized_word.append(vocab_dict.get(i,\"<UNK>\"))\n",
    "    tokenized_words.extend(tokenized_word)\n",
    "        # print(vocab_dict[i])\n",
    "# for word,token in zip(final_words,tokenized_words):\n",
    "#     print(word,\"==>\",token)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(tokenized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
