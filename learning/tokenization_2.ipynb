{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: b'<|endoftext|>', 1: b'\\x00', 2: b'\\x01', 3: b'\\x02', 4: b'\\x03', 5: b'\\x04', 6: b'\\x05', 7: b'\\x06', 8: b'\\x07', 9: b'\\x08', 10: b'\\t', 11: b'\\n', 12: b'\\x0b', 13: b'\\x0c', 14: b'\\r', 15: b'\\x0e', 16: b'\\x0f', 17: b'\\x10', 18: b'\\x11', 19: b'\\x12', 20: b'\\x13', 21: b'\\x14', 22: b'\\x15', 23: b'\\x16', 24: b'\\x17', 25: b'\\x18', 26: b'\\x19', 27: b'\\x1a', 28: b'\\x1b', 29: b'\\x1c', 30: b'\\x1d', 31: b'\\x1e', 32: b'\\x1f', 33: b' ', 34: b'!', 35: b'\"', 36: b'#', 37: b'$', 38: b'%', 39: b'&', 40: b\"'\", 41: b'(', 42: b')', 43: b'*', 44: b'+', 45: b',', 46: b'-', 47: b'.', 48: b'/', 49: b'0', 50: b'1', 51: b'2', 52: b'3', 53: b'4', 54: b'5', 55: b'6', 56: b'7', 57: b'8', 58: b'9', 59: b':', 60: b';', 61: b'<', 62: b'=', 63: b'>', 64: b'?', 65: b'@', 66: b'A', 67: b'B', 68: b'C', 69: b'D', 70: b'E', 71: b'F', 72: b'G', 73: b'H', 74: b'I', 75: b'J', 76: b'K', 77: b'L', 78: b'M', 79: b'N', 80: b'O', 81: b'P', 82: b'Q', 83: b'R', 84: b'S', 85: b'T', 86: b'U', 87: b'V', 88: b'W', 89: b'X', 90: b'Y', 91: b'Z', 92: b'[', 93: b'\\\\', 94: b']', 95: b'^', 96: b'_', 97: b'`', 98: b'a', 99: b'b', 100: b'c', 101: b'd', 102: b'e', 103: b'f', 104: b'g', 105: b'h', 106: b'i', 107: b'j', 108: b'k', 109: b'l', 110: b'm', 111: b'n', 112: b'o', 113: b'p', 114: b'q', 115: b'r', 116: b's', 117: b't', 118: b'u', 119: b'v', 120: b'w', 121: b'x', 122: b'y', 123: b'z', 124: b'{', 125: b'|', 126: b'}', 127: b'~', 128: b'\\x7f', 129: b'\\x80', 130: b'\\x81', 131: b'\\x82', 132: b'\\x83', 133: b'\\x84', 134: b'\\x85', 135: b'\\x86', 136: b'\\x87', 137: b'\\x88', 138: b'\\x89', 139: b'\\x8a', 140: b'\\x8b', 141: b'\\x8c', 142: b'\\x8d', 143: b'\\x8e', 144: b'\\x8f', 145: b'\\x90', 146: b'\\x91', 147: b'\\x92', 148: b'\\x93', 149: b'\\x94', 150: b'\\x95', 151: b'\\x96', 152: b'\\x97', 153: b'\\x98', 154: b'\\x99', 155: b'\\x9a', 156: b'\\x9b', 157: b'\\x9c', 158: b'\\x9d', 159: b'\\x9e', 160: b'\\x9f', 161: b'\\xa0', 162: b'\\xa1', 163: b'\\xa2', 164: b'\\xa3', 165: b'\\xa4', 166: b'\\xa5', 167: b'\\xa6', 168: b'\\xa7', 169: b'\\xa8', 170: b'\\xa9', 171: b'\\xaa', 172: b'\\xab', 173: b'\\xac', 174: b'\\xad', 175: b'\\xae', 176: b'\\xaf', 177: b'\\xb0', 178: b'\\xb1', 179: b'\\xb2', 180: b'\\xb3', 181: b'\\xb4', 182: b'\\xb5', 183: b'\\xb6', 184: b'\\xb7', 185: b'\\xb8', 186: b'\\xb9', 187: b'\\xba', 188: b'\\xbb', 189: b'\\xbc', 190: b'\\xbd', 191: b'\\xbe', 192: b'\\xbf', 193: b'\\xc0', 194: b'\\xc1', 195: b'\\xc2', 196: b'\\xc3', 197: b'\\xc4', 198: b'\\xc5', 199: b'\\xc6', 200: b'\\xc7', 201: b'\\xc8', 202: b'\\xc9', 203: b'\\xca', 204: b'\\xcb', 205: b'\\xcc', 206: b'\\xcd', 207: b'\\xce', 208: b'\\xcf', 209: b'\\xd0', 210: b'\\xd1', 211: b'\\xd2', 212: b'\\xd3', 213: b'\\xd4', 214: b'\\xd5', 215: b'\\xd6', 216: b'\\xd7', 217: b'\\xd8', 218: b'\\xd9', 219: b'\\xda', 220: b'\\xdb', 221: b'\\xdc', 222: b'\\xdd', 223: b'\\xde', 224: b'\\xdf', 225: b'\\xe0', 226: b'\\xe1', 227: b'\\xe2', 228: b'\\xe3', 229: b'\\xe4', 230: b'\\xe5', 231: b'\\xe6', 232: b'\\xe7', 233: b'\\xe8', 234: b'\\xe9', 235: b'\\xea', 236: b'\\xeb', 237: b'\\xec', 238: b'\\xed', 239: b'\\xee', 240: b'\\xef', 241: b'\\xf0', 242: b'\\xf1', 243: b'\\xf2', 244: b'\\xf3', 245: b'\\xf4', 246: b'\\xf5', 247: b'\\xf6', 248: b'\\xf7', 249: b'\\xf8', 250: b'\\xf9', 251: b'\\xfa', 252: b'\\xfb', 253: b'\\xfc', 254: b'\\xfd', 255: b'\\xfe', 256: b'\\xff', 257: b' t', 258: b'he', 259: b' a', 260: b' s', 261: b' w', 262: b'nd', 263: b'ed', 264: b' f', 265: b' h', 266: b' the', 267: b'it', 268: b'er', 269: b'as', 270: b' to', 271: b'in', 272: b' d', 273: b'is', 274: b' b', 275: b'ie', 276: b're', 277: b'll', 278: b'ou', 279: b' and', 280: b'ay', 281: b' was', 282: b' n', 283: b' l', 284: b' T', 285: b' p', 286: b' o', 287: b' th', 288: b'me', 289: b' c', 290: b'at', 291: b' B', 292: b'uc', 293: b'en', 294: b'rie', 295: b'ow', 296: b'ne', 297: b'riend', 298: b' friend', 299: b' The'}\n",
      "[(b' ', b't'), (b'h', b'e'), (b' ', b'a'), (b' ', b's'), (b' ', b'w'), (b'n', b'd'), (b'e', b'd'), (b' ', b'f'), (b' ', b'h'), (b' t', b'he'), (b'i', b't'), (b'e', b'r'), (b'a', b's'), (b' t', b'o'), (b'i', b'n'), (b' ', b'd'), (b'i', b's'), (b' ', b'b'), (b'i', b'e'), (b'r', b'e'), (b'l', b'l'), (b'o', b'u'), (b' a', b'nd'), (b'a', b'y'), (b' w', b'as'), (b' ', b'n'), (b' ', b'l'), (b' ', b'T'), (b' ', b'p'), (b' ', b'o'), (b' t', b'h'), (b'm', b'e'), (b' ', b'c'), (b'a', b't'), (b' ', b'B'), (b'u', b'c'), (b'e', b'n'), (b'r', b'ie'), (b'o', b'w'), (b'n', b'e'), (b'rie', b'nd'), (b' f', b'riend'), (b' T', b'he')]\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.pretokenization_example import find_chunk_boundaries\n",
    "import regex as re\n",
    "\n",
    "sample_tiny_path = \"/Users/prateekmahadevappahavanur/Documents/GitHub/test_task/assignment1-basics/tests/fixtures/tinystories_sample.txt\"\n",
    "\n",
    "\n",
    "def bpe_train(file_path,vocab_size,special_tokens):\n",
    "    \n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "    special_token_pattern = \"|\".join(re.escape(token) for token in special_tokens)\n",
    "    word_freqs = {}\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(\n",
    "            f, 2, \"<|endoftext|>\".encode(\"utf-8\"))\n",
    "            \n",
    "        # The following is a serial implementation, but you can parallelize this \n",
    "        # by sending each start/end pair to a set of processes.\n",
    "        word_freqs = {}\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "            f.seek(start)\n",
    "            chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "            text = re.split(special_token_pattern, chunk)\n",
    "            for segment in text:\n",
    "                if not segment:\n",
    "                    continue\n",
    "                for match in re.finditer(PAT, segment):\n",
    "                    token = match.group()\n",
    "                    token_bytes = tuple(token.encode(\"utf-8\"))\n",
    "                    # print(token,\"---\",token_bytes)\n",
    "                    word_freqs[token_bytes] = word_freqs.get(token_bytes, 0) + 1\n",
    "\n",
    "    def get_count(mapped_list):\n",
    "        count = {}\n",
    "        for test_list,value in mapped_list.items():\n",
    "            test_list = list(test_list)\n",
    "            for a1,a2 in zip(test_list,test_list[1:]):\n",
    "                count[(a1,a2)] = count.get((a1,a2),0) +  value\n",
    "        return count\n",
    "\n",
    "\n",
    "    def merge_pair(test_list,pair,replacement):\n",
    "        i = 0\n",
    "        new_list = []\n",
    "        while i < len(test_list):\n",
    "            if i < len(test_list) - 1 and test_list[i] == pair[0] and test_list[i+1] == pair[1]:\n",
    "                new_list.append(replacement)\n",
    "                i = i+2\n",
    "            else:\n",
    "                new_list.append(test_list[i])\n",
    "                i += 1\n",
    "        return new_list\n",
    "\n",
    "    def get_token_bytes(token_id, vocab_list):\n",
    "        \"\"\"Get the bytes for a token ID\"\"\"\n",
    "        if token_id < 256:\n",
    "            return bytes([token_id])  # Single byte\n",
    "        else:\n",
    "            return vocab_list[token_id]  # Already merged token\n",
    "        \n",
    "    def make_comparable_pair(pair, vocab_list):\n",
    "        \"\"\"Convert pair elements to comparable format\"\"\"\n",
    "        a, b = pair\n",
    "        # Convert to bytes for consistent comparison\n",
    "        if isinstance(a, int):\n",
    "            a_bytes = bytes([a]) if a < 256 else f\"merged_{a}\".encode()\n",
    "        else:\n",
    "            a_bytes = a\n",
    "        if isinstance(b, int):\n",
    "            b_bytes = bytes([b]) if b < 256 else f\"merged_{b}\".encode()\n",
    "        else:\n",
    "            b_bytes = b\n",
    "        return (a_bytes, b_bytes)\n",
    "\n",
    "    # Initialize vocabulary\n",
    "    vocab_list = []\n",
    "    # Special tokens first\n",
    "    for special_token in special_tokens:\n",
    "        vocab_list.append(special_token.encode(\"utf-8\"))\n",
    "    # All 256 bytes\n",
    "    for i in range(256):\n",
    "        vocab_list.append(bytes([i]))\n",
    "\n",
    "    num_merges = vocab_size - len(special_tokens) - 256\n",
    "\n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        # print(f\"----iteration {i}-----\")\n",
    "        pair_counts = get_count(word_freqs)\n",
    "        # Find best pair with lexicographic tie-breaking\n",
    "        best_pair = max(\n",
    "                pair_counts.items(),\n",
    "                key=lambda x: (x[1], make_comparable_pair(x[0], vocab_list)),\n",
    "            )[0]\n",
    "        best_pair\n",
    "\n",
    "        new_token_id = len(vocab_list)\n",
    "\n",
    "        first_bytes = get_token_bytes(best_pair[0], vocab_list)\n",
    "        second_bytes = get_token_bytes(best_pair[1], vocab_list)\n",
    "        merged_bytes = first_bytes + second_bytes\n",
    "\n",
    "        vocab_list.append(merged_bytes)\n",
    "\n",
    "        # print(merged_bytes)\n",
    "\n",
    "        new_word_freqs = {}\n",
    "        for words,freq in word_freqs.items():\n",
    "            merged_list = merge_pair(words, best_pair, new_token_id)\n",
    "            new_word_freqs[tuple(merged_list)] = freq\n",
    "\n",
    "        word_freqs = new_word_freqs\n",
    "        merges.append(best_pair)\n",
    "\n",
    "\n",
    "    # Build final outputs\n",
    "    vocab_dict = {i: token for i, token in enumerate(vocab_list)}\n",
    "\n",
    "    # Build merges_bytes with proper lookup\n",
    "    merges_bytes = []\n",
    "    for first, second in merges:\n",
    "        first_bytes = get_token_bytes(first, vocab_list)\n",
    "        second_bytes = get_token_bytes(second, vocab_list)\n",
    "        merges_bytes.append((first_bytes, second_bytes))\n",
    "\n",
    "    return vocab_dict,merges_bytes\n",
    "\n",
    "vocab_dict,merges_bytes = bpe_train(sample_tiny_path,300,[\"<|endoftext|>\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def square(x):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
